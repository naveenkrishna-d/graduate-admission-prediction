# Identify indices of near zero variance variables in train data
nzv_idx <- nearZeroVar(train.final, saveMetrics = TRUE)$nzv
# Remove near zero variance variables from train, validation, and test data
train.final <- train.final[,-nzv_idx]
valid.final <- valid.final[,-nzv_idx]
test.final <- test.final[,-nzv_idx]
# First, we need to convert the target variable (SalePrice) to be in 100K units
train.final$SalePrice <- train.final$SalePrice / 100000
test.final$SalePrice <- test.final$SalePrice / 100000
valid.final$SalePrice <- valid.final$SalePrice / 100000
# Next, we need to separate the target variable from the predictors
train_y <- train.final$SalePrice
train_x <- train.final[, !(colnames(train.final) %in% "SalePrice")]
dim(train_x)
dim(train.final)
library(keras)
# Define the model
model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(train_x)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
# Compile the model
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(lr = 0.001),
metrics = c("mae", "mse")
)
# Train the model
history <- model %>% fit(
as.matrix(train_x), as.matrix(train_y),
epochs = 100,
batch_size = 32,
validation_split = 0.1,
validation_data=list(as.matrix(train_x),as.matrix(train_y))
)
library(tfruns)
runs= tuning_run("tuning_script.R",
flags=list(
learning_rate=c(0.1,0.5,  0.01, 0.001, 0.0001),
units1=c(8, 16, 32, 64, 128, 512),
units2=c(8, 16, 32, 64, 128),
units3=c(8, 16, 32, 64, 128),
dropout=c(0.1, 0.2, 0.3, 0.4, 0.5),
batch_size=c(8,16, 32)
),
sample= 0.001
)
runs = runs[ order(runs$metric_val_loss),]
runs
view_run(runs$run_dir[8])
#Training run 8/12 (flags = list(0.001, 64, 8, 32, 0.2, 16))
dim(train.final)
dim(valid.final)
train.final <- rbind(train.final,valid.final)
dim(train.final)
dim(valid.final)
#train.final <- rbind(train.final,valid.final)
#SalePrice_target <- train.final$SalePrice/100000
dim(train.final)
dim(valid.final)
dim(train)
library(keras)
final_model <- keras_model_sequential() %>%
layer_dense(units = 64, activation = "relu", input_shape = ncol(train.final)) %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "linear")
final_model %>% compile(
loss = "mse",
optimizer = optimizer_adam(lr = 0.001),
metrics = c("mse")
)
SalePrice_target <- train.final$SalePrice
final_model_fit <- final_model %>% fit(
as.matrix(train.final), as.matrix(SalePrice_target),
epochs = 100,
batch_size = 32,
validation_split = 0.1,
)
# Predict sale prices on the test set
pred_final <- predict(final_model, as.matrix(train.final),na.action = na.pass)
# Multiply predictions and actual sale prices by 100000
test.final$SalePrice <- test.final$SalePrice * 100000
# Compute RMSE
rmse <- sqrt(mean((train.final$SalePrice - pred_final)^2))
rmse
model_list
data(precip)
# calculate the 90% confidence interval
ci <- t.test(precip, conf.level = 0.90)$conf.int
# print the confidence interval
print("confidence interval:", ci)
data(precip)
# calculate the 90% confidence interval
ci <- t.test(precip, conf.level = 0.90)$conf.int
# print the confidence interval
print("confidence interval:"+ ci)
data(precip)
# calculate the 90% confidence interval
ci <- t.test(precip, conf.level = 0.90)$conf.int
# print the confidence interval
print("confidence interval:", ci)
data(precip)
# calculate the 90% confidence interval
ci <- t.test(precip, conf.level = 0.90)$conf.int
# print the confidence interval
cat("confidence interval:", ci)
data(precip)
# calculate the 90% confidence interval
conf <- t.test(precip, conf.level = 0.90)$conf.int
# print the confidence interval
cat("confidence interval:", conf)
Hilbert <- function(n) {
if(n < 2){
stop("n should be greater than 1")
}
H <- matrix(0, n, n)
for (i in 1:n) {
for (j in 1:n) {
H[i, j] <- 1 / (i + j - 1)
}
}
return(H)
}
H <- Hilbert(5)
print(H)
Hilbert <- function(n) {
if(n < 2){
stop("n should be greater than 1")
}
H <- matrix(0, n, n)
for (i in 1:n) {
for (j in 1:n) {
H[i, j] <- 1 / (i + j - 1)
}
}
return(H)
}
H <- Hilbert(5)
print(H)
data(Formaldehyde)
fit <- lm(optden ~ carb + 0, data = Formaldehyde)
summary(fit)
data(precip)
conf <- t.test(precip, conf.level = 0.90)$conf.int
cat("confidence interval:", conf)
Formaldehyde
setwd("~/ML/Final Project")
library(ggcorrplot)
library(lattice)
library(glmnet)
library(mltools)
library(data.table)
library(keras)
library(tfruns)
library(tidyr)
library(GGally)
library(purrr)
library(knitr)
library(scales)
library(psych)
library(dplyr)
library(caret)
library(ggplot2)
data <- read.csv("Admission_Predict.csv")
data
str(data)
summary(data)
data = data[, -c(1)]
data
colSums(is.na(data))
Chances_of_Admission <- data$Chance.of.Admit
options(scipen=999)
# Distribution of Sales Price
hist(data$Chance.of.Admit, col="#0073c2ff",
border = "white",
probability = TRUE,
xlab = "Chances of Admission",
main="Distribution of target Variable (Chance.of.Admit)"
)
# Extract all numerical variables from the original dataframe
numerical_data <- c("GRE.Score", "TOEFL.Score","University.Rating", "SOP", "LOR", "CGPA", "Chance.of.Admit")
#make a dataframe of those numerical variable
data_num <- data[,numerical_data]
# find correlation of all numerical variables
corr<-cor(data_num)
my_corr<-ggcorrplot(corr,hc.order = TRUE, type = "lower",  title = "The association between Continous variables", outline.col = "white")
my_corr + theme(axis.text.x = element_text(size = 10), axis.text.y =
element_text(size = 10))
corr
# Extract all numerical variables from the original dataframe
numerical_data <- c("GRE.Score", "TOEFL.Score", "SOP", "LOR", "CGPA", "Chance.of.Admit")
#make a dataframe of those numerical variable
data_num <- data[,numerical_data]
# find correlation of all numerical variables
corr<-cor(data_num)
my_corr<-ggcorrplot(corr,hc.order = TRUE, type = "lower",  title = "The association between Continous variables", outline.col = "white")
my_corr + theme(axis.text.x = element_text(size = 10), axis.text.y =
element_text(size = 10))
# Create boxplots for each categorical variable
for (var in cat_vars) {
p <- ggplot(cat_df, aes(x = .data[[var]], y = Chance.of.Admit, fill = .data[[var]])) +
geom_boxplot() +
labs(title = paste("Boxplot of Chance.of.Admit by", var), x = var, y = "Chance.of.Admit") +
theme(plot.title = element_text(size = 16), axis.title = element_text(size = 14), axis.text = element_text(size = 12)) +
scale_fill_hue(name = var)
print(p)
}
library(ggcorrplot)
library(lattice)
library(glmnet)
library(mltools)
library(data.table)
library(keras)
library(tfruns)
library(tidyr)
library(GGally)
library(purrr)
library(knitr)
library(scales)
library(psych)
library(dplyr)
library(caret)
library(ggplot2)
data <- read.csv("Admission_Predict.csv")
data
str(data)
summary(data)
data = data[, -c(1)]
data
colSums(is.na(data))
Chances_of_Admission <- data$Chance.of.Admit
options(scipen=999)
# Distribution of Sales Price
hist(data$Chance.of.Admit, col="#0073c2ff",
border = "white",
probability = TRUE,
xlab = "Chances of Admission",
main="Distribution of target Variable (Chance.of.Admit)"
)
# Extract all numerical variables from the original dataframe
numerical_data <- c("GRE.Score", "TOEFL.Score", "SOP", "LOR", "CGPA", "Chance.of.Admit")
#make a dataframe of those numerical variable
data_num <- data[,numerical_data]
# find correlation of all numerical variables
corr<-cor(data_num)
my_corr<-ggcorrplot(corr,hc.order = TRUE, type = "lower",  title = "The association between Continous variables", outline.col = "white")
my_corr + theme(axis.text.x = element_text(size = 10), axis.text.y =
element_text(size = 10))
corr
# Create scatter plots for each numeric variable with Chance.of.Admit
plots <- lapply(numerical_data, function(x) {
ggplot(data, aes(x = !!sym(x), y = Chance.of.Admit)) +
geom_point() +
labs(x = x, y = "Chances of Admission")
})
# Print all plots
for (plot in plots) {
print(plot)
}
df1 = data[, c(numerical_data)]
model <- aov(Chance.of.Admit ~ ., data=df1)
summary(model)
# identify categorical variables
cat_vars <- c("University.Rating", "Research")
cat_df <- data[, c("Chance.of.Admit", cat_vars)]
cat_df$University.Rating <- factor(cat_df$University.Rating)
cat_df$Research <- factor(cat_df$Research)
str(cat_df)
# Create boxplots for each categorical variable
for (var in cat_vars) {
p <- ggplot(cat_df, aes(x = .data[[var]], y = Chance.of.Admit, fill = .data[[var]])) +
geom_boxplot() +
labs(title = paste("Boxplot of Chance.of.Admit by", var), x = var, y = "Chance.of.Admit") +
theme(plot.title = element_text(size = 16), axis.title = element_text(size = 14), axis.text = element_text(size = 12)) +
scale_fill_hue(name = var)
print(p)
}
for (var in cat_vars) {
t <- table(cat_df[[var]], cat_df$Chance.of.Admit)
chisq <- chisq.test(t)
cat(paste0("Chi-squared test for association between Chance.of.Admit and ", var, ":\n"))
print(chisq)
cat("\n")
}
inTrain <- createDataPartition(data$Chance.of.Admit, p = 0.8, list = F)
train_data <-  data[inTrain, ]
test_data <- data[-inTrain, ]
dim(train_data)
dim(test_data)
set.seed(1234)
tr_ctr = trainControl( method = "cv",
number = 10,
repeats = 5,
verboseIter = T
)
lasso <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "glmnet",
tuneGrid = expand.grid(alpha= 1,
lambda = seq(0.01, 0.2, length = 5)),
trControl =  tr_ctr
)
set.seed(1234)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
lasso <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "glmnet",
tuneGrid = expand.grid(alpha= 1,
lambda = seq(0.01, 0.2, length = 5)),
trControl =  tr_ctr
)
set.seed(1234)
tr_ctr = trainControl( method = "cv",
number = 10,
repeats = 5,
verboseIter = T
)
lasso <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "glmnet",
tuneGrid = expand.grid(alpha= 1,
lambda = seq(0.01, 0.2, length = 5)),
trControl =  tr_ctr
)
# print the tuned parameter
lasso$bestTune
lasso
# print the tuned parameter
lasso$bestTune
# get the coefficients for the best tuned model
lasso_coefficient <- coef(lasso$finalModel, s = lasso$bestTune$lambda)
lasso_coefficient
# check if Lasso shrink some of the coefficients to zero
sum(lasso_coefficient[-1] == 0) > 0
plot(varImp(lasso, scale = T  ))
predictions<- predict(lasso,
newdata = test_data,
na.action = na.pass,
)
lasso_rmse <- RMSE(predictions, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Lasso Regression is: ", lasso_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
ridge <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "glmnet",
tuneGrid = expand.grid(alpha= 0,
lambda = seq(0.0001, 1, length = 5)),
trControl =  tr_ctr
)
plot(varImp(ridge, scale = T  ))
predictions<- predict(ridge,
newdata = test_data,
na.action = na.pass,
)
ridge_rmse <- RMSE(predictions, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Ridge Regression is: ", ridge_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
elastic_net <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "glmnet",
tuneGrid = expand.grid(alpha= seq(0,1,length = 10),
lambda = seq(0.0001, 0.2, length = 5)),
trControl =  tr_ctr
)
plot(varImp(elastic_net, scale = T))
predictions<- predict(elastic_net,
newdata = test_data,
na.action = na.pass,
)
en_rmse <- RMSE(predictions, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Elastic Net Regression is: ", en_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
rndmForest <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "rf",
importance = T,
trControl =  tr_ctr
)
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
rndmForest <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "rf",
importance = T,
trControl =  tr_ctr
)
plot(varImp(rndmForest, scale = T))
p_rf<- predict(rndmForest,
newdata = test_data,
na.action = na.pass,
)
rf_rmse <- RMSE(p_rf, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Random Forest Model is: ", rf_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
gbm <- train(
Chance.of.Admit ~ .,
data = train_data,
method = "gbm",
trControl =  tr_ctr
)
p_gbm<- predict(gbm,
newdata = test_data,
na.action = na.pass,
)
gbm_rmse <- RMSE(p_gbm, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Gradient Boosted is: ", gbm_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
svm <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "svmLinear",
importance = T,
trControl =  tr_ctr
)
p_svm <- predict(svm,
newdata = test_data,
na.action = na.pass,
)
svm_rmse <- RMSE(p_svm, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Linear SVM Regression is: ", svm_rmse))
set.seed(1)
tr_ctr = trainControl( method = "repeatedcv",
number = 10,
repeats = 5,
verboseIter = T
)
svmRad <- train(
Chance.of.Admit ~ .,
data = train_data,
na.action = na.pass,
preProcess = c("knnImpute", "nzv"),
method = "svmRadial",
importance = T,
trControl =  tr_ctr
)
p_svmRad <- predict(svmRad,
newdata = test_data,
na.action = na.pass,
)
svmRad_rmse <- RMSE(p_svmRad, test_data$Chance.of.Admit)
cat(paste("The RMSE Value of Radial SVM is: ", svmRad_rmse))
model_list <- list(Lasso_Regression= lasso, Ridge_Regression = ridge, ElasticNet_Regression = elastic_net, RandomForest_Model = rndmForest, GradientBoost_Model = gbm, SVMLinear_Model = svm, SVMRadial_Model = svmRad)
res <- resamples(model_list)
model_list <- list(Lasso_Regression= lasso, Ridge_Regression = ridge, ElasticNet_Regression = elastic_net, RandomForest_Model = rndmForest, GradientBoost_Model = gbm, SVMLinear_Model = svm, SVMRadial_Model = svmRad)
res <- resamples(model_list)
varImp(lasso)
View(rndmForest)
library(caret)
varImp(lasso)
varImp(rndmForest)
View(data)
summary(res)
model_list <- list(Lasso_Regression= lasso, Ridge_Regression = ridge, ElasticNet_Regression = elastic_net, RandomForest_Model = rndmForest, GradientBoost_Model = gbm, SVMLinear_Model = svm, SVMRadial_Model = svmRad)
res <- resamples(model_list)
